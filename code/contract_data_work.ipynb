{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d15350a-a349-41ea-b7d7-903c946eeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dad3fc9-6ef6-4b49-b8af-4d6d306b55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"/home/t/ticeraskin/Desktop/project/Remuszka_Shared/GovRnD/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6f5fd8-c1a5-41c7-aeaf-2ca06a36916c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_with_system_tools(zip_path, outdir):\n",
    "    from shutil import which\n",
    "\n",
    "    # Try 7zip first (handles all ZIP compression methods)\n",
    "    for exe in [\"7z\", \"7za\", \"7zr\"]:\n",
    "        seven = which(exe)\n",
    "        if seven:\n",
    "            cmd = [seven, \"x\", zip_path, f\"-o{outdir}\", \"-y\"]\n",
    "            try:\n",
    "                subprocess.run(cmd, check=True)\n",
    "                print(f\"Extracted with {exe}.\")\n",
    "                return True\n",
    "            except subprocess.CalledProcessError:\n",
    "                pass\n",
    "\n",
    "    # Try unzip (may or may not support your method)\n",
    "    unzip = which(\"unzip\")\n",
    "    if unzip:\n",
    "        try:\n",
    "            subprocess.run([unzip, \"-o\", zip_path, \"-d\", outdir], check=True)\n",
    "            print(\"Extracted with unzip.\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            pass\n",
    "\n",
    "    raise RuntimeError(\"No extractor available (7z/7za/7zr/unzip not found).\")\n",
    "\n",
    "\n",
    "def load_zip_as_dataframes(zip_path):\n",
    "    # Temporary directory for extraction\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "\n",
    "        # Extract using 7-Zip or unzip\n",
    "        extract_with_system_tools(zip_path, temp_dir)\n",
    "\n",
    "        dfs = {}\n",
    "\n",
    "        # Walk extracted files\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for filename in files:\n",
    "                path = os.path.join(root, filename)\n",
    "\n",
    "                # Load file contents\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        text = f.read()\n",
    "                except Exception:\n",
    "                    with open(path, \"rb\") as f:\n",
    "                        # binary fallback\n",
    "                        dfs[filename] = pd.DataFrame({\"bytes\": [f.read()]})\n",
    "                        continue\n",
    "\n",
    "                # Try CSV parsing\n",
    "                buffer = StringIO(text)\n",
    "                try:\n",
    "                    df = pd.read_csv(buffer)\n",
    "                except Exception:\n",
    "                    # fallback: line-per-row\n",
    "                    df = pd.DataFrame({\"text\": text.splitlines()})\n",
    "\n",
    "                dfs[filename] = df\n",
    "\n",
    "        return dfs\n",
    "\n",
    "def merge_yearly_files(dfs, base_name=\"base_file\"):\n",
    "    \"\"\"\n",
    "    dfs: dict {filename: DataFrame}\n",
    "    base_name: the prefix before _YYYY.csv\n",
    "    Returns a concatenated DataFrame with an added column 'year'.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"^{base_name}_(\\d{{4}})\\.csv$\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for filename, df in dfs.items():\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            df = df.copy()\n",
    "            df[\"year\"] = year\n",
    "            frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(f\"No files found matching pattern {base_name}_YYYY.csv\")\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de55f69-fafc-40e9-b037-57141976c986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,96 CPUs Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz (50657),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "1 file, 3521570299 bytes (3359 MiB)\n",
      "\n",
      "Extracting archive: /home/t/ticeraskin/Desktop/project/Remuszka_Shared/GovRnD/data/G Contracts/base_files.zip\n",
      "--\n",
      "Path = /home/t/ticeraskin/Desktop/project/Remuszka_Shared/GovRnD/data/G Contracts/base_files.zip\n",
      "Type = zip\n",
      "Physical Size = 3521570299\n",
      "\n",
      "Everything is Ok\n",
      "\n",
      "Files: 25\n",
      "Size:       16770993250\n",
      "Compressed: 3521570299\n",
      "Extracted with 7za.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n",
      "/tmp/ipykernel_2402580/3614927591.py:56: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(buffer)\n"
     ]
    }
   ],
   "source": [
    "# Bringing in all the contracts\n",
    "## Reading zip files\n",
    "dfs = load_zip_as_dataframes(data_location+\"G Contracts/base_files.zip\")\n",
    "dfs.keys()\n",
    "\n",
    "## Putting all contracts into one file\n",
    "contracts = merge_yearly_files(dfs)\n",
    "\n",
    "## Removing zeros for size concerns\n",
    "contracts_nonzero = contracts[contracts['federal_action_obligation']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01211f34-3a08-4f25-aa36-f8ff8cc1a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2402580/331931318.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  psc_contracts = pd.read_csv(data_location + \"ticeraskin_remuszka.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Loading in the contracts from Lydia\n",
    "psc_contracts = pd.read_csv(data_location + \"G Contracts/ticeraskin_remuszka.csv\")\n",
    "psc_contracts_nonzero = psc_contracts[psc_contracts['federal_action_obligation']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b74ef0-1d4b-4ba6-a93a-7c578879c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the contracts all together\n",
    "contracts_full = pd.merge(\n",
    "    contracts_nonzero,\n",
    "    psc_contracts_nonzero,\n",
    "    on=\"contract_transaction_unique_key\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_nonzero\", \"_psc\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cca9792-20e2-4ef9-94f3-66a047ec9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1196155, 15)\n"
     ]
    }
   ],
   "source": [
    "contracts_full_nonan = contracts_full.dropna(subset=['product_or_service_code'])\n",
    "\n",
    "RD_contracts = contracts_full_nonan[\n",
    "    contracts_full_nonan['product_or_service_code'].str.startswith(\"A\")\n",
    "].copy()\n",
    "\n",
    "print(RD_contracts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a2b7d66b-e0d6-4767-be0d-de4cf2b96475",
   "metadata": {},
   "outputs": [],
   "source": [
    "RD_copy = RD_contracts.copy()   # rename as needed\n",
    "\n",
    "# Make sure contract_date is datetime\n",
    "RD_copy['action_date'] = pd.to_datetime(RD_copy['action_date'])\n",
    "\n",
    "# Sort for deterministic behavior\n",
    "RD_copy = RD_copy.sort_values(['contract_award_unique_key', 'action_date'])\n",
    "\n",
    "# List to hold indices of rows to remove\n",
    "to_remove = set()\n",
    "\n",
    "# Loop through each key group\n",
    "for key, g in RD_copy.groupby('contract_award_unique_key'):\n",
    "    g = g.sort_values('action_date')\n",
    "    g_idx = g.index.tolist()\n",
    "    \n",
    "    for i in range(len(g) - 1):\n",
    "        first = g.iloc[i]\n",
    "        second = g.iloc[i + 1]\n",
    "        \n",
    "        # Must be within 30 days\n",
    "        if abs(second.action_date - first.action_date) <= pd.Timedelta(days=30):\n",
    "            \n",
    "            # Check offsetting rule\n",
    "            if abs(first.federal_action_obligation_nonzero + second.federal_action_obligation_nonzero) < 0.05 * abs(first.federal_action_obligation_nonzero):\n",
    "                to_remove.add(first.name)\n",
    "                to_remove.add(second.name)\n",
    "\n",
    "# Drop all offsetting rows\n",
    "RD_clean = RD_copy.drop(index=to_remove).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2290103e-36e3-4f25-a301-1879094b2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dates are datetime\n",
    "RD_clean['action_date'] = pd.to_datetime(RD_clean['action_date'])\n",
    "\n",
    "# Sort so \"first\" is meaningful\n",
    "RD_clean = RD_clean.sort_values(['contract_award_unique_key', 'action_date'])\n",
    "\n",
    "# Aggregation function\n",
    "RD_clean = (\n",
    "    RD_clean\n",
    "    .groupby('contract_award_unique_key')\n",
    "    .agg(\n",
    "        action_date=('action_date', 'first'),\n",
    "        federal_action_obligation_nonzero=('federal_action_obligation_nonzero', 'sum'),\n",
    "        product_or_service_code=('product_or_service_code', 'first'),\n",
    "        awarding_agency_code=('awarding_agency_code', 'first'),\n",
    "        prime_award_transaction_place_of_performance_county_fips_code=(\n",
    "            'prime_award_transaction_place_of_performance_county_fips_code', \n",
    "            'first'\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "RD_clean = RD_clean[RD_clean['federal_action_obligation_nonzero']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "54f88388-4ea0-4c7e-ad0f-266578023e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_walk = pd.read_csv(data_location + \"Misc/ssa_fips_state_county_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "061d9398-8167-45b1-9f8e-d6202614a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both columns to strings\n",
    "RD_clean[\"prime_award_transaction_place_of_performance_county_fips_code\"] = \\\n",
    "    RD_clean[\"prime_award_transaction_place_of_performance_county_fips_code\"].astype(str)\n",
    "\n",
    "fips_walk[\"fipscounty\"] = fips_walk[\"fipscounty\"].astype(str)\n",
    "\n",
    "# Optional: zero-pad to 5 digits if needed\n",
    "#RD_contracts[\"prime_award_transaction_place_of_performance_county_fips_code\"] = \\\n",
    "#    RD_contracts[\"prime_award_transaction_place_of_performance_county_fips_code\"].str.zfill(5)\n",
    "\n",
    "fips_walk[\"fipscounty\"] = fips_walk[\"fipscounty\"].str.zfill(5)\n",
    "\n",
    "local_merge = pd.merge(RD_clean, fips_walk, left_on = \"prime_award_transaction_place_of_performance_county_fips_code\", right_on = \"fipscounty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e57bcf5e-4920-48f2-ad8b-3261ecb6479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "psc_codes = pd.read_excel(data_location + \"Misc/PSC April 2025.xlsx\")\n",
    "\n",
    "RD_describe = pd.merge(local_merge, psc_codes, left_on = \"product_or_service_code\", right_on = \"PSC CODE\")\n",
    "\n",
    "col = 'PRODUCT AND SERVICE CODE FULL NAME (DESCRIPTION)'\n",
    "\n",
    "# 1. Extract the type part after the dash\n",
    "RD_describe['research_type'] = (\n",
    "    RD_describe[col]\n",
    "    .str.split('-', n=1)\n",
    "    .str[1]\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# 2. Create a single categorical column: basic / applied / other\n",
    "def classify_research(x):\n",
    "    if pd.isna(x):\n",
    "        return \"other\"\n",
    "    if \"basic\" in x:\n",
    "        return \"basic\"\n",
    "    if (\"applied\" in x) or (\"development\" in x):\n",
    "        return \"applied\"\n",
    "    return \"other\"\n",
    "\n",
    "RD_describe['research_category'] = RD_describe['research_type'].apply(classify_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "714f6856-524c-45ab-a7f6-654ba920334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  state year_month research_category  federal_action_obligation_nonzero  \\\n",
      "0    AK 2003-02-01             basic                          151389.64   \n",
      "1    AK 2003-02-01             other                           42925.00   \n",
      "2    AK 2003-09-01             basic                         1186644.00   \n",
      "3    AK 2003-09-01             other                          593322.00   \n",
      "4    AK 2004-02-01             basic                           62700.00   \n",
      "\n",
      "   department_of_defense  department_of_energy  \\\n",
      "0                    0.0                   0.0   \n",
      "1                    0.0                   0.0   \n",
      "2                    0.0                   0.0   \n",
      "3                    0.0                   0.0   \n",
      "4                    0.0                   0.0   \n",
      "\n",
      "   department_of_health_and_human_services  \\\n",
      "0                                      0.0   \n",
      "1                                      0.0   \n",
      "2                                      0.0   \n",
      "3                                      0.0   \n",
      "4                                      0.0   \n",
      "\n",
      "   national_aeronautics_and_space_administration  national_science_foundation  \n",
      "0                                            0.0                          0.0  \n",
      "1                                            0.0                          0.0  \n",
      "2                                            0.0                          0.0  \n",
      "3                                            0.0                          0.0  \n",
      "4                                            0.0                          0.0  \n"
     ]
    }
   ],
   "source": [
    "RD_describe = pd.merge(RD_describe, dfs['crosswalk_awarding_agency.csv'], on = \"awarding_agency_code\")\n",
    "\n",
    "# Ensure action_date is datetime\n",
    "RD_describe['action_date'] = pd.to_datetime(RD_describe['action_date'], errors='coerce')\n",
    "\n",
    "# Create a monthly period\n",
    "RD_describe['year_month'] = RD_describe['action_date'].values.astype('datetime64[M]')\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Denominator: state × month × research_category\n",
    "# -----------------------------\n",
    "agg_state_month_research = (\n",
    "    RD_describe\n",
    "    .groupby(['state', 'year_month', 'research_category'], as_index=False)\n",
    "    ['federal_action_obligation_nonzero']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Agencies we want\n",
    "# -----------------------------\n",
    "selected_agencies = [\n",
    "    \"Department of Defense\",\n",
    "    \"National Aeronautics and Space Administration\",\n",
    "    \"Department of Energy\",\n",
    "    \"Department of Health and Human Services\",\n",
    "    \"National Science Foundation\"\n",
    "]\n",
    "\n",
    "# Keep only those agencies\n",
    "df_agencies = RD_describe[RD_describe['awarding_agency_name'].isin(selected_agencies)].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Aggregate: state × month × research_category × agency\n",
    "# -----------------------------\n",
    "agg_agencies = (\n",
    "    df_agencies\n",
    "    .groupby(['state', 'year_month', 'research_category', 'awarding_agency_name'], as_index=False)\n",
    "    ['federal_action_obligation_nonzero']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Pivot → one column per agency\n",
    "# -----------------------------\n",
    "agency_pivot = agg_agencies.pivot_table(\n",
    "    index=['state', 'year_month', 'research_category'],\n",
    "    columns='awarding_agency_name',\n",
    "    values='federal_action_obligation_nonzero',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Clean names\n",
    "agency_pivot.columns = [\n",
    "    col.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "    for col in agency_pivot.columns\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Merge everything together\n",
    "# -----------------------------\n",
    "final_agg = agg_state_month_research.merge(\n",
    "    agency_pivot,\n",
    "    on=['state', 'year_month', 'research_category'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "final_agg.fillna(0, inplace=True)\n",
    "\n",
    "print(final_agg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "f19b23d5-0189-49d2-90a4-cd7edbb1278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BALANCE ----\n",
    "\n",
    "# Unique states\n",
    "states = final_agg['state'].unique()\n",
    "\n",
    "# Global month range\n",
    "start_month = final_agg['year_month'].min()\n",
    "end_month   = final_agg['year_month'].max()\n",
    "\n",
    "months = pd.period_range(start=start_month, end=end_month, freq='M').to_timestamp()\n",
    "\n",
    "# Research categories\n",
    "categories = ['basic', 'applied', 'other']\n",
    "\n",
    "# Full Cartesian product\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [states, months, categories],\n",
    "    names=['state', 'year_month', 'research_category']\n",
    ")\n",
    "\n",
    "# Reindex to balance panel\n",
    "RD_final = (\n",
    "    final_agg\n",
    "    .set_index(['state', 'year_month', 'research_category'])\n",
    "    .reindex(full_index, fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "RD_final.rename(columns={'federal_action_obligation_nonzero': 'federal_action_obligation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "cc055d60-f69b-4032-8ec8-52c769d3c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CREATE BASIC / APPLIED / OTHER COLUMNS ----\n",
    "\n",
    "# Pivot from long (one row per research_category)\n",
    "# → to wide (three columns: basic, applied, other)\n",
    "RD_wide = (\n",
    "    RD_final\n",
    "    .pivot_table(\n",
    "        index=['state', 'year_month'],\n",
    "        columns='research_category',\n",
    "        values=['federal_action_obligation','department_of_defense',\n",
    "               'department_of_health_and_human_services', 'national_aeronautics_and_space_administration',\n",
    "               'department_of_energy', 'national_science_foundation'],\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure missing categories appear as columns\n",
    "for col in ['basic', 'applied', 'other']:\n",
    "    if col not in RD_wide.columns:\n",
    "        RD_wide[col] = 0\n",
    "\n",
    "# Clean column index (pivot makes a MultiIndex)\n",
    "RD_wide.columns.name = None\n",
    "\n",
    "# This is your final panel dataset\n",
    "RD_panel = RD_wide.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "31a1b290-8269-4785-be58-8879ee0cd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CREATE BASIC / APPLIED / OTHER COLUMNS ----\n",
    "\n",
    "RD_wide = (\n",
    "    RD_final\n",
    "    .pivot_table(\n",
    "        index=['state', 'year_month'],\n",
    "        columns='research_category',\n",
    "        values=[\n",
    "            'federal_action_obligation',\n",
    "            'department_of_defense',\n",
    "            'department_of_health_and_human_services',\n",
    "            'national_aeronautics_and_space_administration',\n",
    "            'department_of_energy',\n",
    "            'national_science_foundation'\n",
    "        ],\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten column MultiIndex: \"name_type\"\n",
    "RD_wide.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col\n",
    "    for col in RD_wide.columns\n",
    "]\n",
    "\n",
    "# Ensure missing categories exist\n",
    "for cat in ['basic', 'applied', 'other']:\n",
    "    for var in [\n",
    "        'federal_action_obligation',\n",
    "        'department_of_defense',\n",
    "        'department_of_health_and_human_services',\n",
    "        'national_aeronautics_and_space_administration',\n",
    "        'department_of_energy',\n",
    "        'national_science_foundation'\n",
    "    ]:\n",
    "        colname = f\"{var}_{cat}\"\n",
    "        if colname not in RD_wide.columns:\n",
    "            RD_wide[colname] = 0\n",
    "\n",
    "rename_map = {\n",
    "    'department_of_defense': 'dod',\n",
    "    'department_of_health_and_human_services': 'hhs',\n",
    "    'national_aeronautics_and_space_administration': 'nasa',\n",
    "    'department_of_energy': 'doe',\n",
    "    'national_science_foundation': 'nsf'\n",
    "}\n",
    "\n",
    "for long, short in rename_map.items():\n",
    "    for cat in ['basic', 'applied', 'other']:\n",
    "        old = f\"{long}_{cat}\"\n",
    "        new = f\"{short}_{cat}\"\n",
    "        if old in RD_wide.columns:\n",
    "            RD_wide.rename(columns={old: new}, inplace=True)\n",
    "\n",
    "# --- Rename federal_action_obligation_* to just basic/applied/other ---\n",
    "for cat in ['basic', 'applied', 'other']:\n",
    "    old = f\"federal_action_obligation_{cat}\"\n",
    "    if old in RD_wide.columns:\n",
    "        RD_wide.rename(columns={old: f\"tot_{cat}\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "RD_panel = RD_wide.copy().rename(columns = {\"state_\":\"state\", \"year_month_\":\"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "dc4436e5-347e-4818-a1ef-89490ab79719",
   "metadata": {},
   "outputs": [],
   "source": [
    "RD_panel.to_csv(data_location + 'G Contracts/state_panel.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
